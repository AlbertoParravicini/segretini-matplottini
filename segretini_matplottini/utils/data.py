from functools import reduce
from typing import Any, Callable, Optional, Union

import numpy as np
import pandas as pd
import scipy.stats
import scipy.stats as st
from jaxtyping import Float
from scipy.stats.mstats import gmean


def get_ci_size(
    x: Float[np.ndarray, "#n"],
    ci: float = 0.95,
    estimator_func: Callable[[Float[np.ndarray, "#n"]], float] = np.mean,
    get_raw_location: bool = False,
) -> tuple[float, float, float]:
    """
    Compute the size of the upper and lower confidence interval for a sequence of values.
    and return the center of the confidence interval, plus the lower and upper sizes.

    :param x: A sequence of numerical data, iterable.
    :param ci: Confidence interval to compute.
    :param estimator_func: Callable applied to the sequence, and used to compute the center of the confidence interval.
    :param get_raw_location: If True, report the values of upper and lower intervals,
        instead of their sizes from the center.
    :return: Size of upper confidence interval, size of lower confidence interval, mean.
    """
    center = estimator_func(x)
    ci_lower, ci_upper = st.t.interval(ci, len(x) - 1, loc=center, scale=st.sem(x))
    if not get_raw_location:
        ci_upper -= center
        ci_lower = -center
    return ci_upper, ci_lower, center


def get_upper_ci_size(
    x: Float[np.ndarray, "#n"], ci: float = 0.95, estimator_func: Callable[[Float[np.ndarray, "#n"]], float] = np.mean
) -> float:
    """
    Compute the size of the upper confidence interval,
    i.e. the size between the top of the bar and the top of the error bar as it is generated by Seaborn.
    Useful for adding labels above error bars, or to create by hand the error bars.

    :param x: A sequence of numerical data, iterable.
    :param ci: Confidence interval to compute.
    :param estimator_func: Callable applied to the sequence, and used to compute the center of the confidence interval.
    :return: Size of upper confidence interval
    """
    return get_ci_size(x, ci, estimator_func=estimator_func)[0]


def remove_outliers_ci(data: Float[np.ndarray, "#n"], sigmas: float = 3) -> Float[np.ndarray, "#m"]:
    """
    Filter a sequence of data by keeping only values within "sigma" standard deviations from the mean.
    This is a simple way to filter outliers, it is more useful to clean data
    for visualizations than for sound statistical analyses.

    :param data: A 1D array of numerical data.
    :param sigmas: Number of standard deviations outside which a value is consider to be an outlier.
    :return: The array without outliers.
    """
    return data[np.abs(st.zscore(data)) < sigmas]


def remove_outliers_iqr(
    data: Float[np.ndarray, "#n"], quantile: float = 0.75, iqr_extension: float = 1.5
) -> Float[np.ndarray, "#m"]:
    """
    Filter a sequence of data by removing outliers looking at the quantiles of the distribution.
    Find quantiles (by default, `Q1` and `Q3`), and interquantile range (by default, `Q3 - Q1`),
    and keep values in `[Q1 - iqr_extension * IQR, Q3 + iqr_extension * IQR]`.
    This is the same range used to identify whiskers in a boxplot (e.g. in Pandas and Seaborn).

    :param data: A 1D array of numerical data.
    :param quantile: Upper quantile value used as filtering threshold.
        Also use `(1 - quantile)` as lower threshold. Should be in `[0.5, 1]`.
    :param iqr_extension: multiplier used to filter out outliers.
    :return: The array without outliers.
    """
    assert quantile >= 0.5 and quantile <= 1
    q1 = np.quantile(data, 1 - quantile)
    q3 = np.quantile(data, quantile)
    iqr = scipy.stats.iqr(data, rng=(100 - 100 * quantile, 100 * quantile))
    return data[(data >= q1 - iqr * iqr_extension) & (data <= q3 + iqr * iqr_extension)]


def find_outliers_right_quantile(
    data: Float[np.ndarray, "#n"], quantile: float = 0.75, iqr_extension: float = 1.5
) -> Float[np.ndarray, "#m"]:
    """
    Filter a sequence of data by removing outliers looking at the quantiles of the distribution.
    Since the distribution is not symmetrical, look just at the right quantile,
    and remove values above the specified quantile multiplier.
    In other words, flag as outliers values such that `data > quantile(data) * iqr_extension`.

    :param data: A 1D array of numerical data.
    :param quantile: Upper quantile value used as filtering threshold.
       Must be in `[0.5, 1]`.
    :param iqr_extension: multiplier used to filter out outliers.
    :return: Boolean array that says which values are outliers
    """
    assert quantile <= 1
    q = np.quantile(data, quantile)
    return data > q * iqr_extension


def _remove_outliers_from_dataframe(
    data: pd.DataFrame,
    column: str,
    remove_outliers_func: Callable[..., pd.Series],
    groupby: Optional[list[str]] = None,
    reset_index: bool = True,
    drop_index: bool = True,
    debug: bool = False,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    Filter a pandas DataFrame by removing outliers from a specified column.
    All rows with outlier values are removed.
    Outliers are identified using the specified `remove_outliers_func`.

    :param data: a pandas DataFrame.
    :param column: Name of the column on which the outlier detection is performed.
    :param remove_outliers_func: Function used to remove outliers on an individual column.
    :param groupby: If not None, perform outlier detection on the data after grouping it
        by the specified set of columns.
    :param reset_index: If True, reset the index after filtering.
    :param drop_index: If True, drop the original index column after reset.
    :param sigmas: Number of standard deviations outside which a value is consider to be an outlier.
    :param debug: If True, print how many outliers have been removed.
    :param kwargs: Additional keywork arguments provided to `remove_outliers_func`.
    :return: Data without outliers.
    """

    def _remove_outliers_df(curr_data: pd.DataFrame, **kwargs: Any) -> pd.DataFrame:
        col = curr_data[column]
        res = curr_data.loc[remove_outliers_func(col, **kwargs).index]
        if reset_index:
            res = res.reset_index(drop=drop_index)
        return res

    old_len = len(data)
    if groupby is None:
        new_data = _remove_outliers_df(data, **kwargs)
    else:
        filtered = []
        for _, g in data.groupby(groupby, sort=False):
            filtered += [_remove_outliers_df(g, **kwargs)]
        new_data = pd.concat(filtered, ignore_index=True)
    if debug and (len(new_data) < old_len):
        print(f"removed {old_len - len(new_data)} outliers")
    return new_data


def remove_outliers_from_dataframe_ci(
    data: pd.DataFrame,
    column: str,
    groupby: Optional[list[str]] = None,
    sigmas: float = 3,
    reset_index: bool = True,
    drop_index: bool = True,
    debug: bool = False,
) -> pd.DataFrame:
    """
    Filter a pandas DataFrame by removing outliers from a specified column.
    All rows with outlier values are removed.
    Outliers are identified as being outside "sigma" standard deviations from the column mean.
    This is a simple way to filter outliers, it is more useful to clean data
    for visualizations than for sound statistical analyses.

    :param data: a pandas DataFrame.
    :param column: Name of the column on which the outlier detection is performed.
    :param remove_outliers_func: Function used to remove outliers on an individual column.
    :param groupby: If not None, perform outlier detection on the data after grouping it
        by the specified set of columns.
    :param sigmas: Number of standard deviations outside which a value is consider to be an outlier.
    :param reset_index: If True, reset the index after filtering.
    :param drop_index: If True, drop the original index column after reset.
    :param debug: If True, print how many outliers have been removed.
    :param kwargs: Additional keywork arguments provided to `remove_outliers_func`.
    :return: Data without outliers.
    """
    return _remove_outliers_from_dataframe(
        data,
        column,
        remove_outliers_func=remove_outliers_ci,
        groupby=groupby,
        reset_index=reset_index,
        drop_index=drop_index,
        debug=debug,
        sigmas=sigmas,
    )


def remove_outliers_from_dataframe_iqr(
    data: pd.DataFrame,
    column: str,
    groupby: Optional[list[str]] = None,
    quantile: float = 0.75,
    iqr_extension: float = 1.5,
    reset_index: bool = True,
    drop_index: bool = True,
    debug: bool = False,
) -> pd.DataFrame:
    """
    Filter a pandas DataFrame by removing outliers from a specified column.
    All rows with outlier values are removed.
    Outliers are identified as being outside "sigma" standard deviations from the column mean.
    This is a simple way to filter outliers, it is more useful to clean data
    for visualizations than for sound statistical analyses.

    :param data: a pandas DataFrame.
    :param column: Name of the column on which the outlier detection is performed.
    :param remove_outliers_func: Function used to remove outliers on an individual column.
    :param groupby: If not None, perform outlier detection on the data after grouping it
        by the specified set of columns.
    :param quantile: Upper quantile value used as filtering threshold.
        Also use `(1 - quantile)` as lower threshold. Should be in `[0.5, 1]`.
    :param iqr_extension: multiplier used to filter out outliers.
    :param reset_index: If True, reset the index after filtering.
    :param drop_index: If True, drop the original index column after reset.
    :param debug: If True, print how many outliers have been removed.
    :param kwargs: Additional keywork arguments provided to `remove_outliers_func`.
    :return: Data without outliers.
    """
    return _remove_outliers_from_dataframe(
        data,
        column,
        remove_outliers_func=remove_outliers_iqr,
        groupby=groupby,
        reset_index=reset_index,
        drop_index=drop_index,
        debug=debug,
        quantile=quantile,
    )


# def compute_speedup(data: pd.DataFrame, col_slow: str, col_fast: str, col_speedup: str) -> pd.DataFrame:
#     """
#     Add a column to a dataframe that represents a speedup,
#     and "col_slow", "col_fast" are execution times (e.g. CPU and GPU execution time).
#     Speedup is computed as `data[col_slow] / data[col_fast]`

#     :param data: A Pandas DataFrame where the speedup is computed.
#     :param col_slow: The baseline column used for the speedup.
#     :param col_fast: The other colum used for the speedup.
#     :param col_speedup: Name of the column where the speedup is stored.
#     :return: The DataFrame.
#     """
#     data[col_speedup] = data[col_slow] / data[col_fast]
#     return data


# def correct_speedup_df(
#     data: pd.DataFrame,
#     groupby: list[str],
#     baseline_filter_col: str,
#     baseline_filter_val: str,
#     speedup_col_name: str = "speedup",
#     speedup_col_name_reference: Optional[str] = None,
# ) -> pd.DataFrame:
#     """
#     Divide the speedups in `speedup_col_name` by the geomean of `speedup_col_name_reference`,
#     grouping values by the columns in `groupby` and specifying a baseline column and value to use as reference.
#     In most cases, `speedup_col_name` and `speedup_col_name_reference` are the same value.
#     Useful to ensure that the geomean baseline speedup is 1, and that the other speedups are corrected to reflect that.

#     1. Divide the data in groups denoted by `groupby`
#     2. For each group, select rows where `data[baseline_filter_col] == baseline_filter_val`
#     3. Compute the geometric mean of the column `speedup_col_name_reference` for the rows selected at (2)
#     4. Divide the values in `speedup_col_name_reference` for the current group selected at (1)
#        by the geometric mean computed at (3)

#     :param data: Input DataFrame.
#     :param groupby: List of columns on which the grouping is performed, e.g. `["benchmark_name", "implementation"]`.
#     :param baseline_filter_col: One or more columns used to recognize the baseline, e.g. `["hardware"]`.
#     :param baseline_filter_val : One or more values in `baseline_filter_col` used
#         to recognize the baseline, e.g. `["cpu"]`.
#     :param speedup_col_name: Name of the speedup column to adjust. The default is `"speedup"`.
#     :param speedup_col_name_reference: Name of the reference speedup column,
#         by default it is the same as `"speedup_col_name"`.
#     :return: The updated DataFrame.
#     """
#     if not speedup_col_name_reference:
#         speedup_col_name_reference = speedup_col_name
#     for _, g in data.groupby(groupby):
#         gmean_speedup = gmean(g.loc[g[baseline_filter_col] == baseline_filter_val, speedup_col_name_reference])
#         data.loc[g.index, speedup_col_name] /= gmean_speedup
#     return data


def compute_speedup_df(
    data: pd.DataFrame,
    groupby: list[str],
    baseline_filter_col: Union[str, list[str]],
    baseline_filter_val: Union[str, list[str]],
    speedup_col_name: str = "speedup",
    time_column_name: str = "exec_time",
    baseline_col_name: str = "baseline_time",
    correction: bool = True,
    aggregation: Callable[[pd.Series], float] = np.median,
    compute_relative_perf: bool = False,
) -> pd.DataFrame:
    """
    Compute speedups on a DataFrame by grouping values.

    1. Divide the data in groups denoted by `groupby`
    2. For each group, select rows where `data[baseline_filter_col] == baseline_filter_val`
    3. Compute the mean of the column `speedup_col_name_reference` for the rows selected at (2)
    4. Divide the values in `speedup_col_name_reference` for the current group selected at (1)
       by the mean computed at (3)

    :param data: Input DataFrame.
    :param groupby: List of columns on which the grouping is performed, e.g. `["benchmark_name", "implementation"]`.
    :param baseline_filter_col: One or more columns used to recognize the baseline, e.g. `["hardware"]`.
    :param baseline_filter_val : One or more values in `baseline_filter_col` used
        to recognize the baseline, e.g. `["cpu"]`.
    :param speedup_col_name: Name of the speedup column to adjust. The default is `"speedup"`.
    :param time_column_name: Name of the execution time column. The default is `"exec_time"`. This is the column
        where the mean performance is computed, and speedup is obtained as a relative execution time.
    :param baseline_col_name: Add a new column where we add the execution time of the baseline used to compute the
        speedup in each group. The default is `"baseline_time"`.
    :param correction : If `True`, ensure that the median of the baseline is 1. The default is `True`.
    :param aggregation: Function used to aggregate values. The default is `np.median`.
    :param compute_relative_perf: If `True`, compute relative performance instead of speedup (i.e. `1 / speedup`);
    :return: The updated DataFrame.
    """

    # Initialize speedup values;
    data[speedup_col_name] = 1
    data[baseline_col_name] = 0

    if isinstance(baseline_filter_col, str):
        baseline_filter_col = [baseline_filter_col]
    if isinstance(baseline_filter_val, str):
        baseline_filter_val = [baseline_filter_val]

    assert len(baseline_filter_col) == len(baseline_filter_val)

    grouped_data = data.groupby(groupby, as_index=False)
    for _, group in grouped_data:
        # Compute the median baseline computation time;
        indices = [group[group[i] == j].index for i, j in zip(baseline_filter_col, baseline_filter_val)]
        reduced_index = reduce(lambda x, y: x.intersection(y), indices)
        group_to_aggregate = data.loc[reduced_index, time_column_name]
        mean_baseline = aggregation(group_to_aggregate)
        # Compute the speedup for this group;
        group.loc[:, speedup_col_name] = (
            (group[time_column_name] / mean_baseline)
            if compute_relative_perf
            else (mean_baseline / group[time_column_name])
        )
        group.loc[:, baseline_col_name] = mean_baseline
        data.loc[group.index, :] = group

        # Guarantee that the geometric mean of speedup referred to the baseline is 1, and adjust speedups accordingly;
        if correction:
            gmean_speedup = gmean(data.loc[reduced_index, speedup_col_name])
            group.loc[:, speedup_col_name] /= gmean_speedup
            data.loc[group.index, :] = group


def compute_relative_performance(
    data: pd.DataFrame,
    category: str,
    value: str,
    baseline_category: str,
    groupby: Optional[list[str]] = None,
    aggregation_function: Callable[[pd.Series], float] = np.mean,
    relative_performance_format_string: Callable[[str], str] = lambda x: f"{x}_relative_performance",
    lower_is_better: bool = False,
    add_baseline_value_to_result: bool = False,
    baseline_value_format_string: Callable[[str], str] = lambda x: f"{x}_baseline_value",
) -> pd.DataFrame:
    """
    Compute the relative performance of categories in a DataFrame when compared to the performance
    of a baseline category.
    The input DataFrame must have a string/categorical column whose name is the value of `category`,
    and a numerical column whose name is the value of `value`.
    Optionally, group data by the columns whose names are the values of `group_by`.
    The baseline category must be one of the values in `category`.

    For example, given the following DataFrame,
    where `category` is `cat`, `value` is `val` and `group_by` is `[groupy_by_1]`:

    ```
    +----------+-----------+-------+
    | cat      | group_by_1| val   |
    +----------+-----------+-------+
    | baseline | A         | 0.1   |
    | model_1  | A         | 0.3   |
    | model_2  | A         | 0.6   |
    | baseline | B         | 2     |
    | model_1  | B         | 4     |
    | model_2  | B         | 6     |
    +----------+-----------+-------+
    ```

    The result will be:

    ```
    +----------+-----------+--------------------------+
    | cat      | group_by_1| val_relative_performance |
    +----------+-----------+--------------------------+
    | baseline | A         | 1                        |
    | model_1  | A         | 3                        |
    | model_2  | A         | 6                        |
    | baseline | B         | 1                        |
    | model_1  | B         | 2                        |
    | model_2  | B         | 3                        |
    +----------+-----------+--------------------------+
    ```

    :param data: A DataFrame with the data for which to compute the relative performance.
    :param category: Name of the column that contains the categories. Values must be strings or Categorical.
    :param value: Name of the column that contains the values. Values must be numerical.
    :param baseline_category: Name of the category that is used as baseline.
        It must be a value that appear in `data[category]`.
    :param groupby: If not None, group data by the columns whose names are the values of `group_by`.
    :param aggregation_function: Function used to aggregate values.
    :param relative_performance_format_string: Function used to format the name of the column
        where the relative performance is stored.
    :param lower_is_better: If True, invert the relative performance in case a metric is better for lower values.
        For example, for execution time we would obtain the speedup.
    :param add_baseline_value_to_result: If True, add a column to the result DataFrame
        where the baseline value is stored.
    :param baseline_value_format_string: Function used to format the name of the column
        where the baseline value is stored.
    :return: A DataFrame with the relative performance.
    """

    # Create a new DataFrame with the relative performance column.
    # Initialize it with the raw values;
    relative_performance_column_name = relative_performance_format_string(value)
    _data = data.rename(columns={value: relative_performance_column_name})
    # Create a column where to store the average baseline value, if requested;
    if add_baseline_value_to_result:
        _data[baseline_value_format_string(value)] = np.nan
    # If not groupby is required, simply obtain the baseline performance, and divide values by it;
    if groupby is None or len(groupby) == 0:
        assert baseline_category in _data[category].values, (
            f"❌ baseline category {baseline_category} not found in data, "
            f"available categories are {set(_data[category].values)}"
        )
        baseline_value = _data.loc[_data[category] == baseline_category, relative_performance_column_name].agg(
            aggregation_function
        )
        _data[relative_performance_column_name] /= baseline_value
        if add_baseline_value_to_result:
            _data[baseline_value_format_string(value)] = baseline_value
    else:
        # Obtain each group, then compute relative performance within each group;
        for _, group in _data.groupby(groupby, sort=False, as_index=False):
            assert baseline_category in group[category].values, (
                f"❌ baseline category {baseline_category} not found in data, "
                f"available categories are {set(group[category].values)}"
            )
            # Compute the aggregated baseline value for the group;
            baseline_value = group.loc[group[category] == baseline_category, relative_performance_column_name].agg(
                aggregation_function
            )
            # Compute the relative performance for this group;
            group.loc[:, relative_performance_column_name] /= baseline_value
            if add_baseline_value_to_result:
                group.loc[:, baseline_value_format_string(value)] = baseline_value
            _data.loc[group.index, :] = group

    # If lower is better, invert the relative performance.
    # For example, for execution time we would obtain the speedup;
    if lower_is_better:
        _data[relative_performance_column_name] = 1 / _data[relative_performance_column_name]
    return _data
