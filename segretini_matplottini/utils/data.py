from dataclasses import dataclass
from typing import Any, Callable, Optional, Union

import numpy as np
import pandas as pd
import scipy.stats
import scipy.stats as st
from jaxtyping import Bool, Float


def get_ci_size(
    x: Float[np.ndarray, "#n"],
    ci: float = 0.95,
    estimator_func: Callable[[Float[np.ndarray, "#n"]], float] = np.mean,
    get_raw_location: bool = False,
) -> tuple[float, float, float]:
    """
    Compute the size of the upper and lower confidence interval
    for the mean (or another location estimator) of a sequence of values.

    The assumption is that the array `x` is contains IID samples from normal distribution
    with unknown standard deviation.

    :param x: A sequence of numerical data, iterable.
    :param ci: Confidence interval to compute.
    :param estimator_func: Callable applied to the sequence, and used to compute the center of the confidence interval.
    :param get_raw_location: If True, report the values of upper and lower intervals,
        instead of their sizes from the center.
    :return: Size of upper confidence interval, size of lower confidence interval, mean.
    """
    center = estimator_func(x)
    ci_lower, ci_upper = st.t.interval(ci, len(x) - 1, loc=center, scale=st.sem(x))
    if not get_raw_location:
        ci_upper -= center
        ci_lower = -center
    return ci_upper, ci_lower, center


def get_upper_ci_size(
    x: Float[np.ndarray, "#n"], ci: float = 0.95, estimator_func: Callable[[Float[np.ndarray, "#n"]], float] = np.mean
) -> float:
    """
    Compute the size of the upper confidence interval for the mean of a sequence `x`,
    i.e. the size between the top of the bar and the top of the error bar as it is generated by Seaborn.
    Useful for adding labels above error bars, or to create by hand the error bars.

    :param x: A sequence of numerical data, iterable.
    :param ci: Confidence interval to compute.
    :param estimator_func: Callable applied to the sequence, and used to compute the center of the confidence interval.
    :return: Size of upper confidence interval
    """
    return get_ci_size(x, ci, estimator_func=estimator_func)[0]


def remove_outliers_ci(data: Float[np.ndarray, "#n"], sigmas: float = 3) -> Float[np.ndarray, "#m"]:
    """
    Filter a sequence of data by keeping only values within "sigma" standard deviations from the mean.
    This is a simple way to filter outliers, it is more useful to clean data
    for visualizations than for sound statistical analyses.

    :param data: A 1D array of numerical data.
    :param sigmas: Number of standard deviations outside which a value is consider to be an outlier.
    :return: The array without outliers.
    """
    return data[np.abs(st.zscore(data)) < sigmas]  # type: ignore


def remove_outliers_iqr(
    data: Float[np.ndarray, "#n"], quantile: float = 0.75, iqr_extension: float = 1.5
) -> Float[np.ndarray, "#m"]:
    """
    Filter a sequence of data by removing outliers looking at the quantiles of the distribution.
    Find quantiles (by default, `Q1` and `Q3`), and interquantile range (by default, `Q3 - Q1`),
    and keep values in `[Q1 - iqr_extension * IQR, Q3 + iqr_extension * IQR]`.
    This is the same range used to identify whiskers in a boxplot (e.g. in Pandas and Seaborn).

    :param data: A 1D array of numerical data.
    :param quantile: Upper quantile value used as filtering threshold.
        Also use `(1 - quantile)` as lower threshold. Should be in `[0.5, 1]`.
    :param iqr_extension: multiplier used to filter out outliers.
    :return: The array without outliers.
    """
    assert quantile >= 0.5 and quantile <= 1
    q1 = np.quantile(data, 1 - quantile)
    q3 = np.quantile(data, quantile)
    iqr = scipy.stats.iqr(data, rng=(100 - 100 * quantile, 100 * quantile))
    return data[(data >= q1 - iqr * iqr_extension) & (data <= q3 + iqr * iqr_extension)]  # type: ignore


def find_outliers_right_quantile(
    data: Float[np.ndarray, "#n"], quantile: float = 0.75, iqr_extension: float = 1.5
) -> Bool[np.ndarray, "#m"]:
    """
    Filter a sequence of data by removing outliers looking at the quantiles of the distribution.
    Since the distribution is not symmetrical, look just at the right quantile,
    and remove values above the specified quantile multiplier.
    In other words, flag as outliers values such that `data > quantile(data) * iqr_extension`.

    :param data: A 1D array of numerical data.
    :param quantile: Upper quantile value used as filtering threshold.
       Must be in `[0.5, 1]`.
    :param iqr_extension: multiplier used to filter out outliers.
    :return: Boolean array that says which values are outliers
    """
    assert quantile <= 1
    q = np.quantile(data, quantile)
    return data > q * iqr_extension  # type: ignore


def _remove_outliers_from_dataframe(
    data: pd.DataFrame,
    column: str,
    remove_outliers_func: Callable[..., pd.Series],
    groupby: Optional[list[str]] = None,
    reset_index: bool = True,
    drop_index: bool = True,
    verbose: bool = False,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    Filter a pandas DataFrame by removing outliers from a specified column.
    All rows with outlier values are removed.
    Outliers are identified using the specified `remove_outliers_func`.

    :param data: a pandas DataFrame.
    :param column: Name of the column on which the outlier detection is performed.
    :param remove_outliers_func: Function used to remove outliers on an individual column.
    :param groupby: If not None, perform outlier detection on the data after grouping it
        by the specified set of columns.
    :param reset_index: If True, reset the index after filtering.
    :param drop_index: If True, drop the original index column after reset.
    :param sigmas: Number of standard deviations outside which a value is consider to be an outlier.
    :param verbose: If True, print how many outliers have been removed.
    :param kwargs: Additional keywork arguments provided to `remove_outliers_func`.
    :return: Data without outliers.
    """

    def _remove_outliers_df(curr_data: pd.DataFrame, **kwargs: Any) -> pd.DataFrame:
        col = curr_data[column]
        res = curr_data.loc[remove_outliers_func(col, **kwargs).index]
        if reset_index:
            res = res.reset_index(drop=drop_index)
        return res

    old_len = len(data)
    if groupby is None:
        new_data = _remove_outliers_df(data, **kwargs)
    else:
        filtered = []
        for _, g in data.groupby(groupby, sort=False):
            filtered += [_remove_outliers_df(g, **kwargs)]
        new_data = pd.concat(filtered, ignore_index=True)
    if verbose and (len(new_data) < old_len):
        print(f"👉 removed {old_len - len(new_data)} outliers")
    return new_data


def remove_outliers_from_dataframe_ci(
    data: pd.DataFrame,
    column: str,
    groupby: Optional[list[str]] = None,
    sigmas: float = 3,
    reset_index: bool = True,
    drop_index: bool = True,
    verbose: bool = False,
) -> pd.DataFrame:
    """
    Filter a pandas DataFrame by removing outliers from a specified column.
    All rows with outlier values are removed.
    Outliers are identified as being outside "sigma" standard deviations from the column mean.
    This is a simple way to filter outliers, it is more useful to clean data
    for visualizations than for sound statistical analyses.

    :param data: a pandas DataFrame.
    :param column: Name of the column on which the outlier detection is performed.
    :param remove_outliers_func: Function used to remove outliers on an individual column.
    :param groupby: If not None, perform outlier detection on the data after grouping it
        by the specified set of columns.
    :param sigmas: Number of standard deviations outside which a value is consider to be an outlier.
    :param reset_index: If True, reset the index after filtering.
    :param drop_index: If True, drop the original index column after reset.
    :param verbose: If True, print how many outliers have been removed.
    :param kwargs: Additional keywork arguments provided to `remove_outliers_func`.
    :return: Data without outliers.
    """
    return _remove_outliers_from_dataframe(
        data,
        column,
        remove_outliers_func=remove_outliers_ci,
        groupby=groupby,
        reset_index=reset_index,
        drop_index=drop_index,
        verbose=verbose,
        sigmas=sigmas,
    )


def remove_outliers_from_dataframe_iqr(
    data: pd.DataFrame,
    column: str,
    groupby: Optional[list[str]] = None,
    quantile: float = 0.75,
    iqr_extension: float = 1.5,
    reset_index: bool = True,
    drop_index: bool = True,
    verbose: bool = False,
) -> pd.DataFrame:
    """
    Filter a pandas DataFrame by removing outliers from a specified column.
    All rows with outlier values are removed.
    Outliers are identified as being outside "sigma" standard deviations from the column mean.
    This is a simple way to filter outliers, it is more useful to clean data
    for visualizations than for sound statistical analyses.

    :param data: a pandas DataFrame.
    :param column: Name of the column on which the outlier detection is performed.
    :param remove_outliers_func: Function used to remove outliers on an individual column.
    :param groupby: If not None, perform outlier detection on the data after grouping it
        by the specified set of columns.
    :param quantile: Upper quantile value used as filtering threshold.
        Also use `(1 - quantile)` as lower threshold. Should be in `[0.5, 1]`.
    :param iqr_extension: multiplier used to filter out outliers.
    :param reset_index: If True, reset the index after filtering.
    :param drop_index: If True, drop the original index column after reset.
    :param debug: If True, print how many outliers have been removed.
    :param kwargs: Additional keywork arguments provided to `remove_outliers_func`.
    :return: Data without outliers.
    """
    return _remove_outliers_from_dataframe(
        data,
        column,
        remove_outliers_func=remove_outliers_iqr,
        groupby=groupby,
        reset_index=reset_index,
        drop_index=drop_index,
        verbose=verbose,
        quantile=quantile,
        iqr_extension=iqr_extension,
    )


def compute_relative_performance(
    data: pd.DataFrame,
    category: str,
    value: str,
    baseline_category: str,
    groupby: Optional[list[str]] = None,
    aggregation_function: Union[str, Callable[[pd.Series], float]] = "mean",
    relative_performance_format_string: Callable[[str], str] = lambda x: f"{x}_relative_performance",
    lower_is_better: bool = False,
    add_baseline_value_to_result: bool = False,
    baseline_value_format_string: Callable[[str], str] = lambda x: f"{x}_baseline_value",
) -> pd.DataFrame:
    """
    Compute the relative performance of categories in a DataFrame when compared to the performance
    of a baseline category.
    The input DataFrame must have a string/categorical column whose name is the value of `category`,
    and a numerical column whose name is the value of `value`.
    Optionally, group data by the columns whose names are the values of `group_by`.
    The baseline category must be one of the values in `category`.

    For example, given the following DataFrame,
    where `category` is `cat`, `value` is `val` and `group_by` is `[groupy_by_1]`:

    ```
    +----------+-----------+-------+
    | cat      | group_by_1| val   |
    +----------+-----------+-------+
    | baseline | A         | 0.1   |
    | model_1  | A         | 0.3   |
    | model_2  | A         | 0.6   |
    | baseline | B         | 2     |
    | model_1  | B         | 4     |
    | model_2  | B         | 6     |
    +----------+-----------+-------+
    ```

    The result will be:

    ```
    +----------+-----------+--------------------------+
    | cat      | group_by_1| val_relative_performance |
    +----------+-----------+--------------------------+
    | baseline | A         | 1                        |
    | model_1  | A         | 3                        |
    | model_2  | A         | 6                        |
    | baseline | B         | 1                        |
    | model_1  | B         | 2                        |
    | model_2  | B         | 3                        |
    +----------+-----------+--------------------------+
    ```

    :param data: A DataFrame with the data for which to compute the relative performance.
    :param category: Name of the column that contains the categories. Values must be strings or Categorical.
    :param value: Name of the column that contains the values. Values must be numerical.
    :param baseline_category: Name of the category that is used as baseline.
        It must be a value that appear in `data[category]`.
    :param groupby: If not None, group data by the columns whose names are the values of `group_by`.
    :param aggregation_function: Function used to aggregate values.
        Either a string representing an aggregation supported by Pandas ("mean", "median", ...) or a Callable
        that can be applied to a Pandas Series.
    :param relative_performance_format_string: Function used to format the name of the column
        where the relative performance is stored.
    :param lower_is_better: If True, invert the relative performance in case a metric is better for lower values.
        For example, for execution time we would obtain the speedup.
    :param add_baseline_value_to_result: If True, add a column to the result DataFrame
        where the baseline value is stored.
    :param baseline_value_format_string: Function used to format the name of the column
        where the baseline value is stored.
    :return: A DataFrame with the relative performance.
    """

    # Create a new DataFrame with the relative performance column.
    # Initialize it with the raw values;
    relative_performance_column_name = relative_performance_format_string(value)
    _data = data.rename(columns={value: relative_performance_column_name})
    # Create a column where to store the average baseline value, if requested;
    if add_baseline_value_to_result:
        _data[baseline_value_format_string(value)] = np.nan
    # If not groupby is required, simply obtain the baseline performance, and divide values by it;
    if groupby is None or len(groupby) == 0:
        assert baseline_category in _data[category].values, (
            f"❌ baseline category {baseline_category} not found in data, "
            f"available categories are {set(_data[category].values)}"
        )
        baseline_value = _data.loc[_data[category] == baseline_category, relative_performance_column_name].agg(
            aggregation_function
        )
        _data[relative_performance_column_name] /= baseline_value
        if add_baseline_value_to_result:
            _data[baseline_value_format_string(value)] = baseline_value
    else:
        # Obtain each group, then compute relative performance within each group;
        for _, group in _data.groupby(groupby, sort=False, as_index=False):
            assert baseline_category in group[category].values, (
                f"❌ baseline category {baseline_category} not found in data, "
                f"available categories are {set(group[category].values)}"
            )
            # Compute the aggregated baseline value for the group;
            baseline_value = group.loc[group[category] == baseline_category, relative_performance_column_name].agg(
                aggregation_function
            )
            # Compute the relative performance for this group;
            group.loc[:, relative_performance_column_name] /= baseline_value
            if add_baseline_value_to_result:
                group.loc[:, baseline_value_format_string(value)] = baseline_value
            _data.loc[group.index, :] = group

    # If lower is better, invert the relative performance.
    # For example, for execution time we would obtain the speedup;
    if lower_is_better:
        _data[relative_performance_column_name] = 1 / _data[relative_performance_column_name]
    return _data


def true_positives(
    logits: Float[np.ndarray, "#n"], targets: Float[np.ndarray, "#n"], classification_threshold: float
) -> int:
    p = (
        (logits >= classification_threshold).astype(bool)
        if classification_threshold < 1
        else np.zeros(len(logits)).astype(bool)
    )
    t = targets.astype(bool)
    return int((p & t).sum())


def true_negatives(
    logits: Float[np.ndarray, "#n"], targets: Float[np.ndarray, "#n"], classification_threshold: float
) -> int:
    p = (
        (logits >= classification_threshold).astype(bool)
        if classification_threshold < 1
        else np.zeros(len(logits)).astype(bool)
    )
    t = targets.astype(bool)
    return int((~p & ~t).sum())


def false_positives(
    logits: Float[np.ndarray, "#n"], targets: Float[np.ndarray, "#n"], classification_threshold: float
) -> int:
    p = (
        (logits >= classification_threshold).astype(bool)
        if classification_threshold < 1
        else np.zeros(len(logits)).astype(bool)
    )
    t = targets.astype(bool)
    return int((p & ~t).sum())


def false_negatives(
    logits: Float[np.ndarray, "#n"], targets: Float[np.ndarray, "#n"], classification_threshold: float
) -> int:
    p = (
        (logits >= classification_threshold).astype(bool)
        if classification_threshold < 1
        else np.zeros(len(logits)).astype(bool)
    )
    t = targets.astype(bool)
    return int((~p & t).sum())


@dataclass(frozen=True)
class ConfusionMatrix:
    tp: int
    fp: int
    fn: int
    tn: int


def confusion_matrix(
    logits: Float[np.ndarray, "#n"], targets: Float[np.ndarray, "#n"], classification_threshold: float
) -> ConfusionMatrix:
    p = (
        (logits >= classification_threshold).astype(bool)
        if classification_threshold < 1
        else np.zeros(len(logits)).astype(bool)
    )
    t = targets.astype(bool)
    tp = (p & t).sum()
    tn = (~p & ~t).sum()
    fp = (p & ~t).sum()
    fn = (~p & t).sum()
    return ConfusionMatrix(
        tp=tp,
        fp=fp,
        fn=fn,
        tn=tn,
    )
